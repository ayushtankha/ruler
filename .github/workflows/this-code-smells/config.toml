# Codex configuration for weekly repo review via GitHub Models

# Select model and provider
model = "openai/gpt-5"
model_provider = "github-models"

# Reasoning settings for GPT-5 family (Responses API)
model_reasoning_effort = "medium"   # minimal | low | medium | high
model_verbosity = "high"            # low | medium | high

# Keep the CLI quiet and avoid storing history in CI
hide_agent_reasoning = false

# Non-interactive approval policy for CI
approval_policy = "never"

# Surface detailed reasoning traces and summaries in output (if supported by the model/provider)
show_raw_agent_reasoning = true
model_reasoning_summary = "detailed"   # auto | concise | detailed | none
model_supports_reasoning_summaries = true

[history]
persistence = "none"

# Custom provider for GitHub Models (OpenAI-compatible endpoint)
[model_providers."github-models"]
name = "GitHub Models"
base_url = "https://models.inference.ai.azure.com"
# Codex will read the token from this environment variable
env_key = "GITHUB_TOKEN"
# Use Chat Completions API for GitHub Models to avoid unsupported Responses API
wire_api = "chat"

# Network tuning: tolerate rate limits and transient drops
request_max_retries = 6
stream_max_retries = 20
stream_idle_timeout_ms = 600000

# File scanning preferences
[paths]
root = "."
include = ["**/*"]
exclude = [
  ".git/**",
  "node_modules/**",
  "dist/**",
  "build/**",
  "coverage/**",
  "target/**",
  "vendor/**",
  ".venv/**",
  ".env",
  ".cache/**",
  "out/**",
  ".next/**",
  ".turbo/**",
  ".yarn/**",
  "tests/**",
  "img/**",
  "**/*.gif",
  "**/*.png",
  "**/*.jpg",
  "yarn.lock",
  "package-lock.json",
  "pnpm-lock.yaml",
  ".github/workflows/this-code-smells/*.md"
]
